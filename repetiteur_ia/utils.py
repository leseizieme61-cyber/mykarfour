import openai
from django.conf import settings
import os
from datetime import datetime
import json
import tempfile
from django.core.files.storage import FileSystemStorage

# Configuration du client OpenAI
def get_openai_client():
    """Retourne le client OpenAI configur√©"""
    return openai.OpenAI(api_key=settings.OPENAI_API_KEY)

def generer_contenu_ia(titre, matiere, eleve):
    """
    Version r√©elle avec l'API OpenAI
    """
    try:
        prompt = f"""
        Tu es un r√©p√©titeur p√©dagogique expert. 
        Cr√©e une le√ßon sur le sujet "{titre}" dans la mati√®re {matiere} 
        pour un √©l√®ve de {eleve.get_niveau_display()} {eleve.get_classe_display()}.
        
        La le√ßon doit inclure:
        1. Une introduction au sujet
        2. Les concepts cl√©s expliqu√©s clairement
        3. Des exemples concrets adapt√©s au niveau
        4. Une section d'exercices pratiques
        5. Un r√©sum√© des points importants
        
        Formatte le r√©sultat en HTML basique.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un professeur expert, clair et p√©dagogique."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration IA: {e}")
        return f"<p>Contenu temporairement indisponible pour {titre} en {matiere}.</p>"

def generer_salutation_eleve(eleve):
    """
    Retourne une salutation courte pour l'√©l√®ve avec fallback
    """
    try:
        if not getattr(settings, 'OPENAI_API_KEY', None):
            raise RuntimeError("OPENAI_API_KEY non configur√©e")

        # V√©rifier si la cl√© API est valide
        if settings.OPENAI_API_KEY.startswith('sk-proj-'):
            # Cl√© probablement invalide ou sans cr√©dits
            raise RuntimeError("Cl√© API sans cr√©dits")
        
        nom = getattr(eleve.user, 'first_name', '') or getattr(eleve.user, 'username', '√©l√®ve')
        niveau = getattr(eleve, 'get_niveau_display', lambda: '')()
        
        prompt = f"√âcris une salutation courte et chaleureuse en fran√ßais pour l'√©l√®ve {nom}"
        if niveau:
            prompt += f", niveau {niveau}"
        prompt += ". Garde la salutation en une phrase."

        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un assistant bienveillant et p√©dagogique."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=60,
            temperature=0.6
        )
        salutation = response.choices[0].message.content.strip()
        return salutation
        
    except Exception as e:
        print(f"Erreur g√©n√©ration salutation: {e}")
        # Fallback simple et fiable
        try:
            nom = getattr(eleve.user, 'first_name', '') or getattr(eleve.user, 'username', '√©l√®ve')
        except Exception:
            nom = '√©l√®ve'
        
        salutations_fallback = [
            f"Bonjour {nom} ! Pr√™t¬∑e pour une session de r√©vision avec MrKarfour ?",
            f"Salut {nom} ! Bienvenue dans votre espace d'apprentissage.",
            f"Bien le bonjour {nom} ! Votre r√©p√©titeur MrKarfour est √† votre service.",
            f"Enchant√© {nom} ! Commen√ßons cette session p√©dagogique."
        ]
        import random
        return random.choice(salutations_fallback)

def repondre_au_repetiteur(question, contexte_pedagogique=None, contexte_session=None, 
                          niveau_eleve="secondaire", historique_conversation=""):
    """
    Version am√©lior√©e avec contexte de session, historique et fallback robuste
    """
    try:
        if not getattr(settings, 'OPENAI_API_KEY', None):
            raise RuntimeError("API key non configur√©e")

        # Construction du contexte p√©dagogique
        contexte_text = ""
        if contexte_pedagogique and contexte_pedagogique.get('contenus_similaires'):
            contexte_text = "CONTEXTE P√âDAGOGIQUE DISPONIBLE:\n"
            for i, contenu in enumerate(contexte_pedagogique['contenus_similaires'][:3], 1):  # Limiter √† 3 contenus
                contexte_text += f"{i}. {contenu}\n\n"
        
        # Construction du contexte de session
        contexte_session_text = ""
        if contexte_session:
            matiere = contexte_session.get('matiere', 'Non sp√©cifi√©e')
            objectifs = contexte_session.get('objectifs', 'Aucun objectif sp√©cifique')
            soumissions_count = len(contexte_session.get('soumissions', []))
            
            contexte_session_text = f"""
CONTEXTE DE SESSION:
- Mati√®re en cours: {matiere}
- Objectifs: {objectifs}
- Documents soumis: {soumissions_count}
"""
        
        # Construction du contexte historique
        contexte_historique_text = ""
        if historique_conversation:
            contexte_historique_text = f"""
HISTORIQUE R√âCENT DE LA CONVERSATION:
{historique_conversation}

CONSIGNES POUR L'HISTORIQUE:
- Prends en compte cet historique pour maintenir la coh√©rence
- √âvite les r√©p√©titions des explications d√©j√† donn√©es
- Fais des liens avec les sujets pr√©c√©demment abord√©s
- Si l'√©l√®ve revient sur un point d√©j√† discut√©, approfondis ou donne une nouvelle perspective
- Utilise l'historique pour mieux comprendre le niveau et les besoins de l'√©l√®ve
"""
        
        # Construction du prompt am√©lior√©
        prompt = f"""
Tu es MrKarfour, un r√©p√©titeur p√©dagogique bienveillant pour des √©l√®ves de {niveau_eleve}.

{contexte_historique_text}

{contexte_session_text}

{contexte_text}

QUESTION ACTUELLE DE L'√âL√àVE:
"{question}"

TA MISSION:
1. R√©ponds de fa√ßon claire et adapt√©e au niveau {niveau_eleve}
2. Si une session est active, relie ta r√©ponse aux objectifs de r√©vision
3. Utilise le contexte p√©dagogique si pertinent
4. Prends en compte l'historique de conversation pour:
   - Maintenir la coh√©rence
   - √âviter les r√©p√©titions inutiles
   - Faire des liens avec les √©changes pr√©c√©dents
   - Adapter ton approche en fonction du profil de l'√©l√®ve
5. Sois encourageant et p√©dagogique
6. Utilise des exemples concrets si n√©cessaire
7. Si la question fait suite √† un √©change pr√©c√©dent, fais explicitement le lien
8. Termine par une question ouverte pour encourager la poursuite du dialogue

R√âPONSE (en fran√ßais, naturelle et conversationnelle, en maintenant une continuit√© avec l'historique):
"""
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system", 
                    "content": """Tu es MrKarfour, un r√©p√©titeur p√©dagogique exceptionnel. 
                    Tes qualit√©s: bienveillant, patient, clair, encourageant.
                    Tu adaptes toujours tes explications au niveau de l'√©l√®ve.
                    Tu es sp√©cialis√© dans l'aide aux r√©visions et l'explication des concepts difficiles.
                    Tu gardes en m√©moire l'historique des conversations pour fournir des r√©ponses coh√©rentes
                    et √©viter les r√©p√©titions tout en approfondissant les sujets."""
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            max_tokens=1000,
            temperature=0.7
        )

        reponse = response.choices[0].message.content.strip()
        return reponse

    except Exception as e:
        print(f"[ERREUR IA R√©p√©titeur]: {e}")
        # R√©ponse de fallback contextuelle avec prise en compte de l'historique
        if contexte_session and contexte_session.get('matiere'):
            matiere = contexte_session['matiere']
            return f"Bonjour ! Je suis MrKarfour. Pour votre question sur {matiere}, je suis actuellement en cours de configuration. En attendant, n'h√©sitez pas √† explorer vos documents de cours pour {matiere} !"
        else:
            # Essayer de personnaliser m√™me en fallback
            if historique_conversation:
                return f"Bonjour ! Je vois que nous avons d√©j√† √©chang√©. Pour votre question '{question[:50]}...', je suis temporairement en maintenance. Je me souviens de notre conversation pr√©c√©dente et serai bient√¥t de retour pour poursuivre !"
            else:
                return f"Bonjour ! Je suis MrKarfour, votre r√©p√©titeur IA. Pour votre question '{question[:50]}...', je suis actuellement en cours de configuration. En attendant, n'h√©sitez pas √† explorer vos cours et exercices !"

def transcrire_audio(fichier_audio):
    """
    Convertit la voix de l'√©l√®ve en texte gr√¢ce √† Whisper
    """
    try:
        if not getattr(settings, 'OPENAI_API_KEY', None) or settings.OPENAI_API_KEY.startswith('sk-proj-'):
            return "Fonctionnalit√© audio temporairement indisponible. Veuillez taper votre question."
        
        # Sauvegarder le fichier temporairement
        fs = FileSystemStorage()
        filename = fs.save(fichier_audio.name, fichier_audio)
        file_path = fs.path(filename)
        
        # Transcrire avec Whisper
        with open(file_path, "rb") as audio_file:
            transcript = get_openai_client().audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="text",
                language="fr"  # Sp√©cifier le fran√ßais pour de meilleurs r√©sultats
            )
        
        # Nettoyer le fichier temporaire
        fs.delete(filename)
        
        return transcript.strip()

    except Exception as e:
        print(f"[ERREUR TRANSCRIPTION]: {e}")
        return "Je n'ai pas compris la question audio. Pouvez-vous r√©p√©ter ou √©crire votre question ?"

def generer_audio(texte):
    """
    Transforme la r√©ponse texte en audio via TTS
    """
    try:
        if not getattr(settings, 'OPENAI_API_KEY', None) or settings.OPENAI_API_KEY.startswith('sk-proj-'):
            return ""  # Retourner une cha√Æne vide si pas d'API fonctionnelle
        
        if not texte or len(texte.strip()) < 10:
            return ""
            
        # Conversion du texte en parole
        response = get_openai_client().audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=texte[:1000]  # Limiter la longueur
        )
        
        # Sauvegarder dans un fichier temporaire
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
            response.stream_to_file(tmp_file.name)
            return tmp_file.name

    except Exception as e:
        print(f"[ERREUR AUDIO]: {e}")
        return ""

def generer_quiz_ia(cours):
    """
    Version r√©elle avec l'API OpenAI pour g√©n√©rer un quiz
    """
    try:
        prompt = f"""
        En te basant sur le cours suivant: {cours.titre} en {cours.matiere},
        g√©n√®re un quiz de 5 questions avec 4 options de r√©ponse chaque et indique la r√©ponse correcte.
        
        Format attendu: une liste JSON o√π chaque √©l√©ment a:
        - "question": le texte de la question
        - "options": une liste de 4 options
        - "reponse_correcte": l'option correcte (exactement comme dans la liste)
        
        Retourne uniquement le JSON, sans autre texte.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un expert en cr√©ation de quiz p√©dagogiques."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.5
        )
        
        questions = json.loads(response.choices[0].message.content)
        return questions
    
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration du quiz: {e}")
        return []

def _analyser_intention_question(question):
    """
    Analyse l'intention p√©dagogique derri√®re la question
    """
    question_lower = question.lower()
    
    intentions = {
        'explication': ['explique', 'comment', 'pourquoi', 'qu\'est-ce que', 'd√©finition', 'que veut dire', 'signifie'],
        'exercice': ['exercice', 'probl√®me', 'calcul', 'r√©soudre', 'application', 'calcule', 'r√©sous'],
        'methode': ['m√©thode', 'technique', 'proc√©dure', 'comment faire', '√©tapes', 'marche √† suivre'],
        'revision': ['r√©vision', 'rappel', 'r√©viser', 'pr√©paration', 'r√©p√®te', 'rappel'],
        'correction': ['corriger', 'erreur', 'faux', 'juste', 'correct', 'v√©rifie'],
        'approfondissement': ['aller plus loin', 'approfondir', 'en savoir plus', 'd√©taill√©'],
        'exemple': ['exemple', 'exemples', 'cas concret', 'illustration'],
        'comparaison': ['diff√©rence', 'comparer', 'contraire', 'oppos√©', 'similaire']
    }
    
    for intention, mots_cles in intentions.items():
        if any(mot in question_lower for mot in mots_cles):
            return intention
            
    return 'explication'

def analyser_contenu_soumission(contenu_texte, matiere, niveau_eleve):
    """
    Analyse le contenu soumis par l'√©l√®ve pour en extraire les points cl√©s
    """
    try:
        prompt = f"""
        Analyse ce contenu p√©dagogique en {matiere} pour un √©l√®ve de {niveau_eleve} et identifie les √©l√©ments suivants:
        
        CONTENU √Ä ANALYSER:
        {contenu_texte[:2000]}
        
        TON ANALYSE DOIT IDENTIFIER:
        1. Les 3-5 concepts principaux abord√©s
        2. Les d√©finitions importantes
        3. Les formules ou th√©or√®mes cl√©s (si applicable)
        4. Les exemples significatifs
        5. Les difficult√©s potentielles pour un √©l√®ve de ce niveau
        
        Format de r√©ponse: une liste structur√©e et concise en fran√ßais.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un expert en analyse de contenu p√©dagogique."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.3
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur analyse contenu: {e}")
        return "Analyse automatique temporairement indisponible."

def generer_plan_revision_session(session, soumissions):
    """
    G√©n√®re un plan de r√©vision personnalis√© bas√© sur la session et les soumissions
    """
    try:
        matiere = session.emploi_temps.matiere
        objectifs = session.objectifs
        contenu_soumissions = "\n".join([s.contenu_texte for s in soumissions if s.contenu_texte])
        
        prompt = f"""
        Cr√©e un plan de r√©vision de {session.duree_prevue} minutes pour une session de {matiere}.
        
        CONTEXTE:
        - Objectifs: {objectifs}
        - Contenu soumis par l'√©l√®ve: {contenu_soumissions[:1000]}
        - Dur√©e disponible: {session.duree_prevue} minutes
        
        STRUCTURE ATTENDUE:
        1. R√©vision des concepts de base (X minutes)
        2. Exercices d'application (X minutes) 
        3. Points difficiles √† retravailler (X minutes)
        4. Synth√®se et v√©rification (X minutes)
        
        Sois pr√©cis dans la r√©partition du temps et propose des activit√©s concr√®tes.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un expert en planification de r√©visions p√©dagogiques."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600,
            temperature=0.5
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur g√©n√©ration plan r√©vision: {e}")
        return f"Plan de r√©vision standard pour {matiere}:\n1. R√©vision des bases (15min)\n2. Exercices pratiques (20min)\n3. Synth√®se (10min)"

def generer_suggestions_exercices(matiere, niveau_eleve, concepts_cles):
    """
    G√©n√®re des suggestions d'exercices adapt√©s au niveau et aux concepts
    """
    try:
        prompt = f"""
        Propose 3 exercices adapt√©s pour un √©l√®ve de {niveau_eleve} en {matiere}.
        
        Concepts √† travailler: {concepts_cles}
        
        Pour chaque exercice, indique:
        - L'√©nonc√© clair
        - Le niveau de difficult√© (Facile, Moyen, Difficile)
        - Les comp√©tences travaill√©es
        - Un indice pour aider l'√©l√®ve si besoin
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un cr√©ateur d'exercices p√©dagogiques."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.6
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur g√©n√©ration exercices: {e}")
        return f"Exercices standards pour {matiere}:\n1. Exercice d'application basique\n2. Probl√®me contextualis√©\n3. Question de r√©flexion"

def evaluer_comprehension_eleve(reponses_eleve, questions_posees):
    """
    √âvalue la compr√©hension de l'√©l√®ve bas√©e sur ses r√©ponses
    """
    try:
        prompt = f"""
        √âvalue la compr√©hension d'un √©l√®ve bas√©e sur ses r√©ponses:
        
        QUESTIONS POS√âES: {questions_posees}
        R√âPONSES DE L'√âL√àVE: {reponses_eleve}
        
        Donne une √©valuation avec:
        - Points forts identifi√©s
        - Points √† retravailler
        - Suggestions pour progresser
        - Score global de compr√©hension (1-10)
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un √©valuateur p√©dagogique bienveillant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.4
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur √©valuation compr√©hension: {e}")
        return "√âvaluation automatique temporairement indisponible."

# Fonctions de fallback am√©lior√©es
def generer_contenu_ia_fallback(titre, matiere, niveau="secondaire"):
    return f"""
    <div class="prose max-w-none">
        <h2 class="text-2xl font-bold text-gray-800 mb-4">{titre}</h2>
        <p class="text-gray-600 mb-4">Mati√®re: <strong>{matiere}</strong> | Niveau: <strong>{niveau}</strong></p>
        
        <div class="bg-blue-50 border-l-4 border-blue-500 p-4 mb-6">
            <p class="text-blue-700">
                <strong>üìö Contenu en pr√©paration</strong><br>
                Nos experts p√©dagogiques pr√©parent actuellement le contenu pour cette le√ßon. 
                En attendant, vous pouvez:
            </p>
            <ul class="list-disc list-inside mt-2 text-blue-600">
                <li>Consulter vos documents de cours</li>
                <li>R√©viser les chapitres pr√©c√©dents</li>
                <li>Poser des questions sp√©cifiques √† MrKarfour</li>
            </ul>
        </div>
    </div>
    """

def get_salutation_fallback(eleve):
    """Fallback robuste pour les salutations"""
    try:
        nom = getattr(eleve.user, 'first_name', '') or getattr(eleve.user, 'username', '√©l√®ve')
    except:
        nom = '√©l√®ve'
    
    salutations = [
        f"Bonjour {nom} ! Votre r√©p√©titeur MrKarfour est pr√™t √† vous aider.",
        f"Salut {nom} ! Commen√ßons cette session d'apprentissage.",
        f"Bienvenue {nom} ! Je suis MrKarfour, votre assistant p√©dagogique.",
        f"Enchant√© {nom} ! Pr√™t¬∑e pour une s√©ance de r√©vision ?"
    ]
    import random
    return random.choice(salutations)

# Fonction utilitaire pour nettoyer les r√©ponses IA
def nettoyer_reponse_ia(reponse):
    """
    Nettoie et formate la r√©ponse de l'IA pour l'affichage
    """
    if not reponse:
        return "Je n'ai pas pu g√©n√©rer de r√©ponse pour le moment. Veuillez r√©essayer."
    
    # Supprimer les √©ventuels pr√©fixes ind√©sirables
    prefixes = ["MrKarfour:", "Assistant:", "R√©ponse:"]
    for prefix in prefixes:
        if reponse.startswith(prefix):
            reponse = reponse[len(prefix):].strip()
    
    return reponse

# Nouvelle fonction pour analyser l'historique et en extraire le contexte
def analyser_historique_conversation(historique_conversations):
    """
    Analyse l'historique des conversations pour en extraire les th√®mes r√©currents
    et le niveau de compr√©hension de l'√©l√®ve
    """
    if not historique_conversations:
        return ""
    
    try:
        # Pr√©parer le texte de l'historique
        historique_text = "\n".join([
            f"√âchange {i+1}: Q: {conv['question']} | R: {conv['reponse'][:200]}..."
            for i, conv in enumerate(historique_conversations[-5:])  # Derniers 5 √©changes
        ])
        
        prompt = f"""
        Analyse cet historique de conversation entre un √©l√®ve et son r√©p√©titeur IA:
        
        {historique_text}
        
        Identifie:
        1. Les th√®mes ou sujets r√©currents
        2. Le niveau de compr√©hension apparent de l'√©l√®ve
        3. Les difficult√©s persistantes
        4. Les int√©r√™ts manifest√©s
        5. Le style d'apprentissage (ex: besoin d'exemples, de sch√©mas, etc.)
        
        Donne une analyse concise en fran√ßais.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un expert en analyse des interactions p√©dagogiques."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0.4
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur analyse historique: {e}")
        return ""

# Fonction pour g√©n√©rer un r√©sum√© de session bas√© sur l'historique
def generer_resume_session(historique_conversations, objectifs_session):
    """
    G√©n√®re un r√©sum√© de ce qui a √©t√© accompli pendant la session
    """
    if not historique_conversations:
        return "Aucun √©change enregistr√© pendant cette session."
    
    try:
        historique_text = "\n".join([
            f"- {conv['question']} ‚Üí {conv['reponse'][:100]}..."
            for conv in historique_conversations
        ])
        
        prompt = f"""
        R√©sume les accomplissements de cette session de r√©vision bas√©e sur cet historique:
        
        OBJECTIFS INITIAUX: {objectifs_session}
        
        √âCHANGES PENDANT LA SESSION:
        {historique_text}
        
        Cr√©e un r√©sum√© qui:
        1. Liste les concepts abord√©s
        2. √âvalue la progression par rapport aux objectifs
        3. Identifie les points √† retravailler
        4. Donne des recommandations pour la prochaine session
        
        Format: r√©sum√© structur√© en fran√ßais.
        """
        
        response = get_openai_client().chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un expert en synth√®se p√©dagogique."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.5
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"Erreur g√©n√©ration r√©sum√© session: {e}")
        return f"Session termin√©e. {len(historique_conversations)} √©changes r√©alis√©s."

def extraire_texte_fichier(chemin_fichier):
    """
    Extrait le texte d'un fichier (PDF, DOCX, etc.)
    √Ä impl√©menter selon vos besoins
    """
    try:
        # Exemple basique pour les fichiers texte
        if chemin_fichier.endswith('.txt'):
            with open(chemin_fichier, 'r', encoding='utf-8') as f:
                return f.read()
        
        # Pour d'autres formats, vous pouvez utiliser des biblioth√®ques comme:
        # - PyPDF2 pour les PDF
        # - python-docx pour les DOCX
        # - etc.
        
        return f"Contenu du fichier: {os.path.basename(chemin_fichier)}"
        
    except Exception as e:
        print(f"Erreur extraction texte fichier: {e}")
        return f"Fichier: {os.path.basename(chemin_fichier)}"